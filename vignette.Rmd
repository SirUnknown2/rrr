---
title: "Vignette"
output: md_document
---

```{r message = FALSE}
devtools::install_github("chrisaddy/rrr")
```

### Reduced-Rank Regression and Its Special Cases

The multivariate linear regression model
```{r}
library(dplyr)
library(MMST)
library(plotly)
library(gridExtra)
```

$$
\begin{aligned*}
\overset{s \times 1}{\mathbf{Y}} = \overset{s \times 1}{\boldsymbol{\mu}} + \overset{s \times r}{\mathbf{C}} \overset{r \times 1}{\mathbf{X}} + \overset{s \times 1}{\varepsilon}
\end{aligned*}
$$

## Principal Components Analysis

Set

$$
\begin{aligned*}
	\mathbf{Y} & \equiv \mathbf{X} \\
	s & = r \\
	\mathbf{\Gamma} & = \mathbf{I}_r \\
\end{aligned*}
$$
	
Then, the least-squares error criterion

\begin{equation}
	\begin{aligned}
	\mathrm{E}\left[\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B}\mathbf{X}\right)^\tau\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B}\mathbf{X}\right)\right]
	\end{aligned}
\end{equation}

is minimized by the reduced-rank regression solution,

\begin{equation}
	\begin{aligned}
		\mathbf{A}^{\left(t\right)} & = \left(\mathbf{v}_1, \dots, \mathbf{v}_t\right) \\
		\mathbf{B}^{\left(t\right)} & = \mathbf{A}^{\left(t\right) \tau} \\
		\boldsymbol{\mu}^{\left(t\right)} & = \left(\mathbf{I}_r - \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\right)\boldsymbol{\mu}_X \\
	\end{aligned}
\end{equation}

where $\mathbf{v}_j = \mathbf{v}_j \left(\mathbf{\Sigma}_{XX}\right)$ is eigenvector associated with the $j$th largest eigenvalue of  $\mathbf{\Sigma}_{XX}.$

The best reduced-rank approximation to the original $\mathbf{X}$ is

\begin{equation}
	\begin{aligned}
		\hat{\mathbf{X}}^{\left(t\right)} & =
		\boldsymbol{\mu}^{\left(t\right)} + \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)} \mathbf{X}
	\end{aligned}
\end{equation}

```{r}
data(pendigits)

digits <- as_data_frame(pendigits)
digits_class <- digits %>% select(V35)
digits_features <- digits %>% select(-V35, -V36)

#pca_rank_trace(digits_features)

pca_rank_trace_plot(digits_features)

pc_pairwise_plot(digits_features, class_labels = digits_class)

pc_plot_3D(digits_features, class_labels = digits_class)
```

## Canonical Variate Analysis

$\mathbf{X}, \mathbf{Y}$ are jointly distributed, with

\begin{equation}
	\begin{aligned}
		\mathrm{E}\left\{
		\begin{pmatrix}
			\mathbf{X} \\
			\mathbf{Y} \\
		\end{pmatrix}
		\right\} =
		\begin{pmatrix}
			\boldsymbol{\mu}_X \\
			\boldsymbol{\mu}_Y \\
		\end{pmatrix}, \quad
		\mathrm{cov}\left\{
		\begin{pmatrix}
			\mathbf{X} \\
			\mathbf{Y} \\
		\end{pmatrix}\right\} =
		\begin{pmatrix}
			\mathbf{\Sigma}_{XX} & \mathbf{\Sigma}_{XY} \\
			\mathbf{\Sigma}_{YX} & \mathbf{\Sigma}_{YY} \\
		\end{pmatrix}
	\end{aligned}
\end{equation}

Let $1 \leq t \leq \mathrm{min}\left(r, s\right)$. $\mathbf{X}$ and $\mathbf{Y}$ are linearly projected into new vector variates,

\begin{equation}
	\begin{aligned}
		\boldsymbol{\xi}_{t \times 1} = \mathbf{G}_{t \times r} \mathbf{X}_{r \times 1}, \quad \boldsymbol{\omega}_{t \times 1} = \mathbf{H}_{t \times s} \mathbf{X}_{s \times 1}
		\end{aligned}
		\end{equation}

Finding a solution to the problem

\begin{equation}
	\begin{aligned}
		\boldsymbol{\xi} \approx \boldsymbol{\nu} + \boldsymbol{\omega}
	\end{aligned}
\end{equation}

requires minimizing the least-squares criterion

\begin{equation}
	\begin{aligned}
		\mathrm{E}\left[\left(\mathbf{H}\mathbf{Y} - \boldsymbol{\nu} - \mathbf{G}\mathbf{X}\right)\left(\mathbf{H}\mathbf{Y} - \boldsymbol{\nu} - \mathbf{G}\mathbf{X}\right)^\tau\right]
	\end{aligned},

where we assume $\mathrm{cov}\left(\boldsymbol{\omega}\right) = \mathbf{I}_t$.
\end{equation}. This least-squares criterion is minimized when

\begin{equation}
	\begin{aligned}
		\boldsymbol{\nu}^{\left(t\right)} & = \mathbf{H}^{\left(t\right)} \boldsymbol{\mu}_Y - \mathbf{G}^{\left(t\right)}\boldsymbol{\mu}_Y \\
		\mathbf{G}^{\left(t\right)} & =
		\begin{pmatrix}
			\mathbf{v}_1^\tau \\
			\vdots \\
			\mathbf{v}_t^\tau \\
		\end{pmatrix}
		\mathbf{\Sigma}_{YY}^{-1/2} \mathbf{\Sigma}_{YX}\mathbf{\Sigma}_{XX}^{-1} \\
		\mathbf{H}^{\left(t\right)} & =
		\begin{pmatrix}
			\mathbf{v}_1^\tau \\
			\vdots \\
			\mathbf{v}_t^\tau \\
		\end{pmatrix}
		\mathbf{\Sigma}_{YY}^{-1/2} \\
	\end{aligned}
\end{equation}

where $\mathbf{v}_j$ is the eigenvector associated with the $j$th largest eigenvalue of

\begin{equation}
	\begin{aligned}
		\mathbf{R} & = \mathbf{\Sigma}_{YY}^{-1/2}\mathbf{\Sigma}_{YX} \mathbf{\Sigma}_{XX}^{-1} \mathbf{\Sigma}_{XY} \mathbf{\Sigma}_{YY}^{-1/2}
	\end{aligned}
\end{equation}

```{r}
### COMBO-17 galaxy data
data(COMBO17)
galaxy <- as_data_frame(COMBO17) %>%
       select(-starts_with("e."), -Nr, -UFS:-IFD) %>%
       na.omit()

galaxy_x <- galaxy %>% select(-Rmag:-chi2red)
galaxy_y <- galaxy %>% select(Rmag:chi2red)

#galaxy_x
#galaxy_y
```

## Linear Discriminant Analysis
