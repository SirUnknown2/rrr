---
title: "rrr for Principal Component Analysis"
author: "Chris Addy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Mathematical Background

### PCA is a Special Case of [Reduced-Rank Regression](rrr.html)

Set

$$
\begin{aligned}
\mathbf{Y} & \equiv \mathbf{X} \\
\mathbf{\Gamma} & = \mathbf{I}_r
\end{aligned}
$$

Then, the least squares criterion

$$
\mathrm{E}\left[\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B} \mathbf{X}\right)\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B} \mathbf{X}\right)^\tau\right]
$$

is minimized when

$$
\begin{aligned}
  \mathbf{A}^{\left(t\right)} & = \left(\mathbf{v}_1, \dots, \mathbf{v}_t\right) \\
  \mathbf{B}^{\left(t\right)} & = \mathbf{A}^{\left(t\right) \tau} \\
  \boldsymbol{\mu}^{\left(t\right)} & = \left(\mathbf{I}_r - \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\right)\boldsymbol{\mu}_X \\
\end{aligned}
$$

where $\mathbf{v}_j = \mathbf{v}_j \left(\mathbf{\Sigma}_{XX}\right)$ is the eigenvector associated with the $j$th largest eigenvalue of  $\mathbf{\Sigma}_{XX}.$

The best reduced-rank approximation to the original $\mathbf{X}$ is

$$
\begin{aligned}
\hat{\mathbf{X}}^{\left(t\right)} & =
  \boldsymbol{\mu}^{\left(t\right)} + \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)} \mathbf{X} \\
  & \mathrm{or} \\
  \hat{\mathbf{X}} & = \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\mathbf{X}_c \\
\end{aligned}
$$

where $\mathbf{X}_c$ is the vector $\mathbf{X}$ after mean-centering.

The first principle component is a latent variable that is a linear combination of the $X_i$s that maximizes the variance among the $X_i$s. The second principle component is another linear combination that maximizes the variance among the $X_i$s subject to being independent of the first principal component. There are $r$ possible principal components, each independent of each other, that capture decreasing amounts of variance. The goal is to use as few principle components as necessary to capture the variance in the data and reduce dimensionality. The question of how many principle components to keep is equivalent to assessing the effective dimensionality $t$ of the reduced-rank regression.

## The `pendigits` Data Set

```{r message = FALSE, warning = FALSE}
data(pendigits)
digits <- as_data_frame(pendigits) %>% select(-V36)

glimpse(digits)
```

```{r}
digits_features <- digits %>% select(-V35)
digits_class <- digits %>% select(V35)
```

We can get a good visualization of the correlation structure using `GGally::ggcorr`. Below we see that there is very heavy correlation among the variables.

```{r}
GGally::ggcorr(digits_features)
```

## Assessing Dimensionality

The ratio

$$
\frac{\lambda_{t + 1} + \cdots \lambda_r}{\lambda_1 + \cdots \lambda_r}
$$

is a goodness-of-fit measure of how well the last $r - t$ principal components explain the totoal variation in $\mathbf{X}$

The function `pca()` (see below) outputs this goodness-of-fit measure

```{r}
pca(digits_features)$goodness_of_fit
```

### Estimate $t$ and ridge constant $k$ with `rank_trace()`

```{r}
rank_trace(digits_features, digits_features, type = "pca")
```

```{r}
rank_trace(digits_features, digits_features, type = "pca", plot = FALSE)
```

## Plot Principal Component Scores 

### Pairwise Plots with `pairwise_plot()`

```{r}
args(pairwise_plot)
```

A common PCA method of visualization for diagnostic and analysis purposes is to plot the $j$th sample PC scores against the $k$th PC scores,

$$
\begin{aligned}
  \left(\xi_{ij}, \xi_{ik}\right) & \\
  = \left(\hat{\mathbf{v}}_j^\tau \mathbf{X}_i, \hat{\mathbf{v}}_k^\tau \mathbf{X}_i\right)&, \quad i = 1,2, \dots, n  
\end{aligned}
$$

Since the first two principal components will capture the most variance -- and hence the most useful information -- of all possible pairs of principal components, we typically would set $j = 1, k = 2$ and plot the first two sample PC scores against each other. In `rrr` this is the default.

```{r}
#pairwise_plot(digits_features, digits_features, class_labels = digits_class, type = "pca")
```

We can set the $x$- and $y$-axes to whichever pairs of PC scores we would like to plot by changing the `pc_x` and `pc_y` arguments.

```{r}
#pairwise_plot(digits_features, pc_x = 1, pc_y = 3, class_labels = digits_class)
```

### Plot all pairs of PC scores with `pca_allpairs_plot()`

Alternatively, we can look at structure in the data by plotting all PC pairs, along with some other visual diagnostics with `pca_allpairs_plot()`. Along with plotting principal component scores against each other, the plot matrix also shows histograms and box plots to show how the points are distributed along principal component axes.

```{r}
#args(pca_allpairs_plot)
```

```{r}
pca_allpairs_plot(digits_features, rank = 3, class_labels = digits_class)
```

## Fitting a PCA Model

### Fit model with `rrr()`

```{r}
args(pca)
```

```{r}
rrr(digits_features, digits_features, type = "pca", rank  = 3)
```