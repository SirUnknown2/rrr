<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>rrr for Multivariate Regression &bull; rrr</title><!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous"><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">rrr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="http://github.com/chrisaddy/rrr">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>rrr for Multivariate Regression</h1>
                        <h4 class="author">Chris Addy</h4>
            
            <h4 class="date">2016-12-15</h4>
          </div>

    
    
<div class="contents">
<p>This vignette assumes some familiarity on the part of the reader with principal component analysis<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, canonical variate analysis<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>, and/or linear discriminant analysis<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>. For a more thorough treatment, see the appropriate references. Here we will show a brief mathematical motivation for reduced-rank regression and then show that principal component analysis, canonical variate analysis, and linear discriminant analysis are special cases of reduced-rank regression. For a more thorough treatment, see <em>Modern Multivariate Statistical Techniques</em>.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rrr)</code></pre></div>
<div id="classical-multivariate-regression" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#classical-multivariate-regression" class="anchor"> </a></body></html>Classical Multivariate Regression</h2>
<p>Let <span class="math inline">\(\mathbf{X} = \left(X_1, X_2, \dots, X_r\right)^\tau\)</span> and <span class="math inline">\(\mathbf{Y} = \left(Y_1, Y_2, \dots, Y_s\right)^\tau\)</span> be jointly distributed random vectors with</p>
<p><span class="math display">\[
\mathrm{E}\left\{
  \begin{pmatrix}
    \mathbf{X} \\
    \mathbf{Y} \\
  \end{pmatrix}
  \right\} =
  \begin{pmatrix}
    \boldsymbol{\mu}_X \\
    \boldsymbol{\mu}_Y \\
  \end{pmatrix},
  \quad
  \mathrm{cov}\left\{
    \begin{pmatrix}
      \mathbf{X} \\
      \mathbf{Y} \\
    \end{pmatrix}
  \right\} =
  \begin{pmatrix}
    \mathbf{\Sigma}_{XX} &amp; \mathbf{\Sigma}_{XY} \\
    \mathbf{\Sigma}_{YX} &amp; \mathbf{\Sigma}_{YY} \\
  \end{pmatrix}
\]</span></p>
<p>The classical multivariate regression model is given by</p>
<p><span class="math display">\[
\overset{s \times 1}{\mathbf{Y}} = \overset{s \times 1}{\boldsymbol{\mu}} + \overset{s \times r}{\mathbf{C}} \; \overset{r \times 1}{\mathbf{X}} + \overset{s \times 1}{\varepsilon}
\]</span></p>
<p>with</p>
<p><span class="math display">\[
\mathrm{E}\left(\varepsilon\right) = \mathbf{0}, \quad \mathrm{cov}\left(\varepsilon\right) = \mathbf{\Sigma}_{\varepsilon \varepsilon}
\]</span></p>
<p>and <span class="math inline">\(\varepsilon\)</span> distributed independently of <span class="math inline">\(\mathbf{X}.\)</span></p>
<p>To estimate <span class="math inline">\(\boldsymbol{\mu}\)</span> and <span class="math inline">\(\mathbf{C}\)</span> we minimize the least-squares criterion</p>
<p><span class="math display">\[
\mathrm{E}\left[\left(\mathbf{Y} - \boldsymbol{\mu} - \mathbf{C} \mathbf{X}\right)\left(\mathbf{Y} - \boldsymbol{\mu} - \mathbf{C}\mathbf{X}\right)^\tau\right],
\]</span></p>
<p>with expecation taken over the joint distribution of <span class="math inline">\(\left(\mathbf{X}^\tau, \mathbf{Y}^\tau\right)\)</span>, with the assumption that <span class="math inline">\(\mathbf{\Sigma}_{XX}\)</span> is nonsingular, and therefore invertible.</p>
<p>This is minimized when</p>
<p><span class="math display">\[
\begin{aligned}
    \boldsymbol{\mu} &amp; = \boldsymbol{\mu}_Y - \mathbf{C} \boldsymbol{\mu}_X \\
    \mathbf{C} &amp; = \mathbf{\Sigma}_{YX} \mathbf{\Sigma}_{XX}^{-1}
\end{aligned}
\]</span></p>
<p>The least-squares estimator of <span class="math inline">\(\mathbf{C}\)</span> is given by</p>
<p><span class="math display">\[
\hat{\mathbf{C}} = \hat{\mathbf{\Sigma}}_{YX} \hat{\mathbf{\Sigma}}_{XX}^{-1}
\]</span></p>
<p>Note that <span class="math inline">\(\mathbf{C}\)</span> &ndash; and hence <span class="math inline">\(\hat{\mathbf{C}}\)</span> &ndash; contains no term that takes into the account the correlation of the <span class="math inline">\(Y_i\)</span>s. This is a surprising result, since we would expect correlation among the responses.</p>
<p>In other words, to find the least-squares estimate <span class="math inline">\(\hat{\mathbf{C}}\)</span> of <span class="math inline">\(\mathbf{C}\)</span>, one need only regress <span class="math inline">\(\mathbf{X}\)</span> separately on each <span class="math inline">\(Y_i\)</span> and concatenate those multiple-regression coefficient vectors into a matrix to construct the estimated coefficient matrix <span class="math inline">\(\hat{\mathbf{C}}\)</span>.</p>
<p>In some very important sense, the classical multivariate regression model is not <em>truly</em> multivariate.</p>
</div>
<div id="the-tobacco-data-set" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#the-tobacco-data-set" class="anchor"> </a></body></html>The <code>tobacco</code> Data Set</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
<span class="kw">data</span>(tobacco)

tobacco &lt;-<span class="st"> </span><span class="kw">as_data_frame</span>(tobacco)</code></pre></div>
<p>We see that the <code>tobacco</code> data set<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> has 9 variables and 25 observations. There are 6 <span class="math inline">\(X_i\)</span> predictor variables &ndash; representing the percentages of nitrogen, chlorine, potassium, phosphorus, calcium, and magnesium, respectively &ndash; and 3 <span class="math inline">\(Y_j\)</span> response variables &ndash; representing cigarette burn rates in inches per 1,000 seconds, percent sugar in the leaf, and percent nicotine in the leaf, respectively.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tobacco_x &lt;-<span class="st"> </span>tobacco %&gt;%
<span class="st">    </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">"X"</span>))

tobacco_y &lt;-<span class="st"> </span>tobacco %&gt;%
<span class="st">    </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">"Y"</span>))</code></pre></div>
<p>Below we see that there is not only correlation among the <span class="math inline">\(X_i\)</span>s but also among the <span class="math inline">\(Y_i\)</span>s. The classical multivariate will not capture that information.</p>
<p>We can get a good idea of the correlation structure using <code>GGally::ggcorr</code>. GGally is a package that extends the functionality of the package <code>ggplot2</code> and has been utilized in <code>rrr</code> to create pairwise plots, as seen below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GGally::<span class="kw">ggcorr</span>(tobacco_x)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-4-1.png" width="672"></p>
<p>We can see correlation between the <span class="math inline">\(X_i\)</span>s, which will be accounted for in the classical multivariate regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GGally::<span class="kw">ggcorr</span>(tobacco_y)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-5-1.png" width="672"></p>
<p>There is clearly correlation in the <span class="math inline">\(Y_i\)</span>s, especially between percent nicotine and percent sugar. Any regression model that we fit should take this into account.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## multivariate regression

x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(tobacco_x)
y &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(tobacco_y)

multivar_reg &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">cov</span>(y, x) %*%<span class="st"> </span><span class="kw">solve</span>(<span class="kw">cov</span>(x)))

## separate multiple regression

lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(y[,<span class="dv">1</span>] ~<span class="st"> </span>x)$coeff
lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(y[,<span class="dv">2</span>] ~<span class="st"> </span>x)$coeff
lm3 &lt;-<span class="st"> </span><span class="kw">lm</span>(y[,<span class="dv">3</span>] ~<span class="st"> </span>x)$coeff</code></pre></div>
<p>As expected, the multivariate coefficients are the same as the multiple regression coefficients of each of the <span class="math inline">\(Y_i\)</span>s</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">multivar_reg</code></pre></div>
<pre><code>##                      Y1.BurnRate Y2.PercentSugar Y3.PercentNicotine
## X1.PercentNitrogen    0.06197282      -4.3186662          0.5521620
## X2.PercentChlorine   -0.16012848       1.3262863         -0.2785609
## X3.PercentPotassium   0.29211810       1.5899470          0.2175877
## X4.PercentPhosphorus -0.65798016      13.9526510         -0.7231067
## X5.PercentCalcium     0.17302593       0.5525913          0.3230914
## X6.PercentMagnesium  -0.42834825      -3.5021083          2.0048603</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cbind</span>(lm1, lm2, lm3)</code></pre></div>
<pre><code>##                               lm1        lm2        lm3
## (Intercept)            1.41113730 13.6329133 -1.5648236
## xX1.PercentNitrogen    0.06197282 -4.3186662  0.5521620
## xX2.PercentChlorine   -0.16012848  1.3262863 -0.2785609
## xX3.PercentPotassium   0.29211810  1.5899470  0.2175877
## xX4.PercentPhosphorus -0.65798016 13.9526510 -0.7231067
## xX5.PercentCalcium     0.17302593  0.5525913  0.3230914
## xX6.PercentMagnesium  -0.42834825 -3.5021083  2.0048603</code></pre>
</div>
<div id="reduced-rank-regression" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#reduced-rank-regression" class="anchor"> </a></body></html>Reduced-Rank Regression</h2>
<p>One way to introduce a multivariate component into the model is to allow for the possibility that <span class="math inline">\(\mathbf{C}\)</span> is deficient, or of <em>reduced-rank</em> <span class="math inline">\(t\)</span>.</p>
<p><span class="math display">\[
\mathrm{rank}\left(\mathbf{C}\right) = t \leq \mathrm{min}\left(r, s\right)
\]</span></p>
<p>In other words, we allow for the possibility that there are unknown linear constraints on <span class="math inline">\(\mathbf{C}\)</span>.</p>
<p>Without loss of generality, we consider the case when <span class="math inline">\(r &gt; s\)</span>, i.e., <span class="math inline">\(t &lt; s\)</span>.</p>
<p>When <span class="math inline">\(t = s\)</span>, the regression model is <em>full-rank</em>, and can be fit using multiple regression on each <span class="math inline">\(Y_i \in \mathbf{Y}\)</span> as seen above. When <span class="math inline">\(t &lt; s\)</span>, <span class="math inline">\(\mathbf{C}\)</span> can be decomposed into non-unique matrices <span class="math inline">\(\mathbf{A}_{s \times t}\)</span> and <span class="math inline">\(\mathbf{B}_{t \times r}\)</span>, such that <span class="math inline">\(\mathbf{C} = \mathbf{AB},\)</span> and the multivariate regression model is given by</p>
<p><span class="math display">\[
\overset{s \times 1}{\mathbf{Y}} = \overset{s \times 1}{\boldsymbol{\mu}} + \overset{s \times t}{\mathbf{A}} \; \overset{t \times r}{\mathbf{B}} \; \overset{r \times 1}{\mathbf{X}} + \overset{s \times 1}{\varepsilon}
\]</span></p>
<p>Estimating <span class="math inline">\(\boldsymbol{\mu}, \mathbf{A}, \mathbf{B}\)</span>, and the <em>reduced-rank regression coefficient</em> <span class="math inline">\(\mathbf{C}^{\left(t\right)}\)</span>, is done by minimizing the weighted sum-of-squares criterion</p>
<p><span class="math display">\[
\mathrm{E}\left[\left(\mathbf{Y} - \boldsymbol{\mu} - \mathbf{ABX}\right)^\tau \mathbf{\Gamma}\left(\mathbf{Y} - \boldsymbol{\mu} - \mathbf{ABX}\right)\right]
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Gamma}\)</span> is a positive-definite symmetric <span class="math inline">\(\left(s \times s\right)\)</span>-matrix of weights. This expectation is taken over the joint distribution <span class="math inline">\(\left(\mathbf{X}^\tau, \mathbf{Y}^\tau\right)^\tau\)</span>. The weighted sum-of-squares criterion is minimized when</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{\mu}^{\left(t\right)} &amp; = \boldsymbol{\mu}_Y - \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\boldsymbol{\mu}_X \\
        \mathbf{A}^{\left(t\right)} &amp; = \mathbf{\Gamma}^{-1/2}\mathbf{V}_t \\
        \mathbf{B}^{\left(t\right)} &amp; = \mathbf{V}_t^\tau \boldsymbol{\Gamma}^{-1/2}\mathbf{\Sigma}_{YX}\mathbf{\Sigma}_{XX}^{-1} \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{V}_t = \left(\mathbf{v}_1, \dots, \mathbf{v}_t\right)\)</span> is an <span class="math inline">\(\left(s \times t\right)\)</span>-matrix, with <span class="math inline">\(\mathbf{v}_j\)</span> the eigenvector associated with the <span class="math inline">\(j\)</span>th largest eigenvalue of</p>
<p><span class="math display">\[
\mathbf{\Gamma}^{1/2}\mathbf{\Sigma}_{YX} \mathbf{\Sigma}_{XX}^{-1} \mathbf{\Sigma}_{XY} \mathbf{\Gamma}^{1/2}
\]</span></p>
<p>We try out different values of <span class="math inline">\(\mathbf{\Gamma}\)</span> in applications. Two popular choices &ndash; and ones that lead to interesting results as we will see &ndash; are <span class="math inline">\(\mathbf{\Gamma} = \mathbf{I}_r\)</span> and <span class="math inline">\(\mathbf{\Gamma} = \boldsymbol{\Sigma}_{YY}^{-1}\)</span>. The following is equivalent to performing canonical variate analysis.</p>
<p>Since the reduced-rank regression coefficient relies on inverting <span class="math inline">\(\boldsymbol{\Sigma}_{XX}\)</span> and, possibly, <span class="math inline">\(\boldsymbol{\Sigma}_{YY}\)</span>, we want to take into consideration the cases when <span class="math inline">\(\boldsymbol{\Sigma}_{XX}, \boldsymbol{\Sigma}_{YY}\)</span> are singular or difficult to invert.</p>
<p>We can perturb the diagonal of the covariance matrices by some small constant, <span class="math inline">\(k\)</span>. This will ensure that the covariance matrix is invertible by &ndash; only slightly &ndash; altering the data. The motivation for is taken from ridge regression<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> in the multiple-regression context, and from the idea of <em>softly-shrunken</em> reduced-rank regression.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> Thus, we carry out the reduced-rank regression procedure using</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\boldsymbol{\Sigma}}_{XX}^{\left(k\right)} &amp; = \hat{\boldsymbol{\Sigma}}_{XX} + k \mathbf{I}_r \\
\hat{\boldsymbol{\Sigma}}_{YY}^{\left(k\right)} &amp; = \hat{\boldsymbol{\Sigma}}_{YY} + k \mathbf{I}_r
\end{aligned}
\]</span></p>
<div id="assessessing-effective-dimensionality" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#assessessing-effective-dimensionality" class="anchor"> </a></body></html>Assessessing Effective Dimensionality</h3>
</div>
<div id="estimate-t-and-k-with-rank_trace-" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#estimate-t-and-k-with-rank_trace-" class="anchor"> </a></body></html>Estimate <span class="math inline">\(t\)</span> and <span class="math inline">\(k\)</span> with <code><a href="../reference/rank_trace.html">rank_trace()</a></code>.</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">args</span>(rank_trace)</code></pre></div>
<pre><code>## function (x, y, type = "identity", k = 0, plot = TRUE, interactive = FALSE) 
## NULL</code></pre>
<p>&nbsp;<span class="math inline">\(\hat{\mathbf{C}}\)</span> is calculated using sample observations. Therefore its <em>mathematical</em> rank will always be full, but it will have a <em>statistical</em> rank <span class="math inline">\(t\)</span> which is an unknown hyperparameter that needs to be estimated.</p>
<p>One method of estimating <span class="math inline">\(t\)</span> is to plot the <em>rank trace</em>. Along the <span class="math inline">\(X\)</span>-axis, we plot a measure of the difference between the rank-<span class="math inline">\(t\)</span> coefficient matrix and the full-rank coefficient matrix for each value of <span class="math inline">\(t\)</span>. Along the <span class="math inline">\(Y\)</span>-axis, we plot the reduction in residual covariance between the rank-<span class="math inline">\(t\)</span> residuals and the full-rank residuals for each value of <span class="math inline">\(t\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### use the identity matrix for gamma

<span class="kw"><a href="../reference/rank_trace.html">rank_trace</a></span>(tobacco_x, tobacco_y)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-9-1.png" width="672"></p>
<p>Set <code>plot = FALSE</code> to print data frame of rank trace coordinates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/rank_trace.html">rank_trace</a></span>(tobacco_x, tobacco_y, <span class="dt">plot =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## # A tibble: 4 &times; 3
##   ranks         dC         dEE
##   &lt;int&gt;      &lt;dbl&gt;       &lt;dbl&gt;
## 1     0 1.00000000 1.000000000
## 2     1 0.20198327 0.011933691
## 3     2 0.08419093 0.003095346
## 4     3 0.00000000 0.000000000</code></pre>
<p>When the weight matrix, <span class="math inline">\(\mathbf{\Gamma}\)</span>, takes on a more complicated form, the rank trace may plot points outside the unit square, or may not be a smooth monotic curve. When this is the case, we can change the value of <code>k</code> to smooth the rank trace. This value of <span class="math inline">\(k\)</span> is then used as the ridge perturbation, <span class="math inline">\(k\)</span>, described above.</p>
</div>
<div id="fitting-reduced-rank-regression-model" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#fitting-reduced-rank-regression-model" class="anchor"> </a></body></html>Fitting Reduced-Rank Regression Model</h3>
<p>The main function in the <code>rrr</code> package is <code><a href="../reference/rrr.html">rrr()</a></code> which fits a reduced-rank regression model and outputs the coefficients.</p>
</div>
<div id="fit-reduced-rank-regression-model-with-rrr" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#fit-reduced-rank-regression-model-with-rrr" class="anchor"> </a></body></html>Fit reduced-rank regression model with <code><a href="../reference/rrr.html">rrr()</a></code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">args</span>(rrr)</code></pre></div>
<pre><code>## function (x, y, type = "identity", rank = "full", k = 0) 
## NULL</code></pre>
<p><code><a href="../reference/rrr.html">rrr()</a></code> takes as inputs the data frames, or matrices, of input and response variables, the type of reduced-rank regression procedure to perform, the rank (defaulted to full rank), and the ridge constant <span class="math inline">\(k\)</span>. The argument <code>type</code> defaults to <code>type = "identity"</code>, which sets <span class="math inline">\(\mathbf{\Gamma} = \mathbf{I}\)</span> but can be set to <code>type = "pca"</code>, <code>type = "cva"</code>, or <code>type = "lda"</code> to perform principal component analysis, canonical variate analysis, or linear discriminant analysis, respectively</p>
<p><code><a href="../reference/rrr.html">rrr()</a></code> outputs the appropriate coefficients depending on the type of reduced-rank regression performed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/rrr.html">rrr</a></span>(tobacco_x, tobacco_y, <span class="dt">rank =</span> <span class="st">"full"</span>)</code></pre></div>
<pre><code>## $mean
##                         [,1]
## Y1.BurnRate         1.411137
## Y2.PercentSugar    13.632913
## Y3.PercentNicotine -1.564824
## 
## $A
##                           [,1]       [,2]      [,3]
## Y1.BurnRate         0.03107787 -0.4704307 0.8818895
## Y2.PercentSugar    -0.97005030  0.1984637 0.1400521
## Y3.PercentNicotine  0.24090782  0.8598297 0.4501736
## 
## $B
##      X1.PercentNitrogen X2.PercentChlorine X3.PercentPotassium
## [1,]          4.3242696        -1.35864835          -1.4808316
## [2,]         -0.4114869         0.09903401           0.3652138
## [3,]         -0.3016163        -0.08086722           0.5782436
##      X4.PercentPhosphorus X5.PercentCalcium X6.PercentMagnesium
## [1,]           -13.729424        -0.4528289          3.86689562
## [2,]             2.456879         0.3060762          1.23030547
## [3,]             1.048309         0.3754285          0.03430168
## 
## $C
##                    X1.PercentNitrogen X2.PercentChlorine
## Y1.BurnRate                0.06197282         -0.1601285
## Y2.PercentSugar           -4.31866620          1.3262863
## Y3.PercentNicotine         0.55216201         -0.2785609
##                    X3.PercentPotassium X4.PercentPhosphorus
## Y1.BurnRate                  0.2921181           -0.6579802
## Y2.PercentSugar              1.5899470           13.9526510
## Y3.PercentNicotine           0.2175877           -0.7231067
##                    X5.PercentCalcium X6.PercentMagnesium
## Y1.BurnRate                0.1730259          -0.4283482
## Y2.PercentSugar            0.5525913          -3.5021083
## Y3.PercentNicotine         0.3230914           2.0048603
## 
## $eigenvalues
## [1] 3.28209974 0.03782978 0.01015996</code></pre>
<p>We can see that <code><a href="../reference/rrr.html">rrr()</a></code> with <code>rank = "full"</code> and <code>k = 0</code> returns the classical multivariate regression coefficients as above. They differ only by a transpose, and is presented this way in <code>rrr</code> as a matter of convention. It is this form that is presented in the literature.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
</div>
<div id="diagnostics" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#diagnostics" class="anchor"> </a></body></html>Diagnostics</h3>
</div>
<div id="plot-and-print-residuals-with-residuals" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#plot-and-print-residuals-with-residuals" class="anchor"> </a></body></html>Plot and Print Residuals with <code><a href="../reference/residuals.html">residuals()</a></code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">args</span>(residuals)</code></pre></div>
<pre><code>## function (x, y, type = "identity", rank = "full", k = 0, plot = TRUE) 
## NULL</code></pre>
</div>
<div id="plot-residuals" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#plot-residuals" class="anchor"> </a></body></html>Plot Residuals</h3>
<p>We can visually check the model assumptions with <code><a href="../reference/residuals.html">residuals()</a></code>. The leftmost column of the scatter plot can be used to look for serial patterns in the residuals. The diagonal can be used to look at the distribution and visually assess whether or not it is symmetric, has a mean of zero, etc.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/residuals.html">residuals</a></span>(tobacco_x, tobacco_y, <span class="dt">rank =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-14-1.png" width="672"></p>
<p>To print a data frame of the residuals, set <code>plot = FALSE</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/residuals.html">residuals</a></span>(tobacco_x, tobacco_y, <span class="dt">rank =</span> <span class="dv">1</span>, <span class="dt">plot =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## # A tibble: 25 &times; 3
##    Y1.BurnRate Y2.PercentSugar Y3.PercentNicotine
##          &lt;dbl&gt;           &lt;dbl&gt;              &lt;dbl&gt;
## 1  -0.15358056       3.9207245         -0.9019766
## 2  -0.20963638       0.6975085         -0.6966471
## 3  -0.07296342       3.3478677         -0.9497452
## 4  -0.20128009       2.9831896         -0.1991789
## 5  -0.12358413       1.6364610         -0.3622142
## 6  -0.04949370       0.3195653         -1.2428487
## 7  -0.04185307       2.0824285         -0.3387954
## 8  -0.14497977       2.7465342         -0.1903404
## 9  -0.09788965       1.5330911         -0.5878621
## 10 -0.35531326       2.6111104         -0.3332054
## # ... with 15 more rows</code></pre>
</div>
</div>
<div id="principal-components-analysis" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#principal-components-analysis" class="anchor"> </a></body></html>Principal Components Analysis</h2>
<div id="pca-is-a-special-case-of-reduced-rank-regression" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#pca-is-a-special-case-of-reduced-rank-regression" class="anchor"> </a></body></html>PCA is a Special Case of Reduced-Rank Regression</h3>
<p>Set</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Y} &amp; \equiv \mathbf{X} \\
\mathbf{\Gamma} &amp; = \mathbf{I}_r
\end{aligned}
\]</span></p>
<p>Then, the least squares criterion</p>
<p><span class="math display">\[
\mathrm{E}\left[\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B} \mathbf{X}\right)\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B} \mathbf{X}\right)^\tau\right]
\]</span></p>
<p>is minimized when</p>
<p><span class="math display">\[
\begin{aligned}
  \mathbf{A}^{\left(t\right)} &amp; = \left(\mathbf{v}_1, \dots, \mathbf{v}_t\right) \\
  \mathbf{B}^{\left(t\right)} &amp; = \mathbf{A}^{\left(t\right) \tau} \\
  \boldsymbol{\mu}^{\left(t\right)} &amp; = \left(\mathbf{I}_r - \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\right)\boldsymbol{\mu}_X \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{v}_j = \mathbf{v}_j \left(\mathbf{\Sigma}_{XX}\right)\)</span> is the eigenvector associated with the <span class="math inline">\(j\)</span>th largest eigenvalue of <span class="math inline">\(\mathbf{\Sigma}_{XX}.\)</span></p>
<p>The best reduced-rank approximation to the original <span class="math inline">\(\mathbf{X}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\mathbf{X}}^{\left(t\right)} &amp; =
  \boldsymbol{\mu}^{\left(t\right)} + \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)} \mathbf{X} \\
  &amp; \mathrm{or} \\
  \hat{\mathbf{X}} &amp; = \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\mathbf{X}_c \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}_c\)</span> is the vector <span class="math inline">\(\mathbf{X}\)</span> after mean-centering.</p>
</div>
</div>
<div id="the-pendigits-data-set" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#the-pendigits-data-set" class="anchor"> </a></body></html>The <code>pendigits</code> Data Set</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(pendigits)
digits &lt;-<span class="st"> </span><span class="kw">as_data_frame</span>(pendigits) %&gt;%<span class="st"> </span><span class="kw">select</span>(-V36)</code></pre></div>
<p>Forty-four writers hand-wrote the digits 0-9 250 times in random order on a in 5 500x500 pixel boxes on a pressure-sensitive tablet with integrated LCD screen. The first 10 digits were thrown out &ndash; without telling the writers &ndash; to ignore variation from the writers gaining familiarity with the device.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> The raw data of the coordinates was cleaned and translated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">digits_features &lt;-<span class="st"> </span>digits %&gt;%<span class="st"> </span><span class="kw">select</span>(-V35)
digits_class &lt;-<span class="st"> </span>digits %&gt;%<span class="st"> </span><span class="kw">select</span>(V35)</code></pre></div>
<p>We can get a good visualization of the correlation structure using <code>GGally::ggcorr</code>. Below we see that there is very heavy correlation among the variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GGally::<span class="kw">ggcorr</span>(digits_features)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-18-1.png" width="672"></p>
<div id="assessing-dimensionality" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#assessing-dimensionality" class="anchor"> </a></body></html>Assessing Dimensionality</h3>
<p>The ratio</p>
<p><span class="math display">\[
\frac{\lambda_{t + 1} + \cdots \lambda_r}{\lambda_1 + \cdots \lambda_r}
\]</span></p>
<p>is a goodness-of-fit measure of how well the last <span class="math inline">\(r - t\)</span> principal components explain the totoal variation in <span class="math inline">\(\mathbf{X}\)</span></p>
<p>The function <code><a href="../reference/rrr.html">rrr()</a></code> (see below) outputs this goodness-of-fit measure</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/rrr.html">rrr</a></span>(digits_features, digits_features, <span class="dt">type =</span> <span class="st">"pca"</span>)$goodness_of_fit</code></pre></div>
<pre><code>##           PC1           PC2           PC3           PC4           PC5 
##  7.168777e-01  4.680599e-01  3.144671e-01  2.243293e-01  1.664009e-01 
##           PC6           PC7           PC8           PC9          PC10 
##  1.181315e-01  8.737814e-02  6.065970e-02  4.138801e-02  2.763994e-02 
##          PC11          PC12          PC13          PC14          PC15 
##  1.896530e-02  1.222037e-02  7.761240e-03  3.816942e-03  1.958674e-03 
##          PC16          PC17          PC18          PC19          PC20 
##  3.133204e-04  1.279543e-04 -6.983068e-17 -8.770990e-17 -9.882538e-17 
##          PC21          PC22          PC23          PC24          PC25 
## -1.092158e-16 -1.158156e-16 -1.172696e-16 -1.184188e-16 -1.145857e-16 
##          PC26          PC27          PC28          PC29          PC30 
## -1.103970e-16 -1.053676e-16 -9.801713e-17 -8.865624e-17 -7.546966e-17 
##          PC31          PC32          PC33          PC34 
## -6.161541e-17 -4.449590e-17 -2.257015e-17  0.000000e+00</code></pre>
</div>
<div id="estimate-t-and-ridge-constant-k-with-rank_trace" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#estimate-t-and-ridge-constant-k-with-rank_trace" class="anchor"> </a></body></html>Estimate <span class="math inline">\(t\)</span> and ridge constant <span class="math inline">\(k\)</span> with <code><a href="../reference/rank_trace.html">rank_trace()</a></code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/rank_trace.html">rank_trace</a></span>(digits_features, digits_features, <span class="dt">type =</span> <span class="st">"pca"</span>)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-20-1.png" width="672"></p>
<p>Print data frame of rank trace coordinates by setting <code>plot = FALSE</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/rank_trace.html">rank_trace</a></span>(digits_features, digits_features, <span class="dt">type =</span> <span class="st">"pca"</span>, <span class="dt">plot =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## # A tibble: 35 &times; 3
##     rank   delta_C delta_residuals
##    &lt;int&gt;     &lt;dbl&gt;           &lt;dbl&gt;
## 1      0 1.0000000      1.00000000
## 2      1 0.9851844      0.71687775
## 3      2 0.9701425      0.46805987
## 4      3 0.9548637      0.31446713
## 5      4 0.9393364      0.22432933
## 6      5 0.9235481      0.16640087
## 7      6 0.9074852      0.11813147
## 8      7 0.8911328      0.08737814
## 9      8 0.8744746      0.06065970
## 10     9 0.8574929      0.04138801
## # ... with 25 more rows</code></pre>
</div>
<div id="plot-principal-component-scores" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#plot-principal-component-scores" class="anchor"> </a></body></html>Plot Principal Component Scores</h3>
</div>
<div id="pairwise-plots-with-pairwise_plot" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#pairwise-plots-with-pairwise_plot" class="anchor"> </a></body></html>Pairwise Plots with <code><a href="../reference/pairwise_plot.html">pairwise_plot()</a></code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">args</span>(pairwise_plot)</code></pre></div>
<pre><code>## function (x, y, type = "pca", pair_x = 1, pair_y = 2, rank = "full", 
##     k = 0, interactive = FALSE, point_size = 2.5) 
## NULL</code></pre>
<p>A common PCA method of visualization is to plot the <span class="math inline">\(j\)</span>th sample PC scores against the <span class="math inline">\(k\)</span>th PC scores,</p>
<p><span class="math display">\[
\begin{aligned}
  \left(\xi_{ij}, \xi_{ik}\right) &amp; \\
  = \left(\hat{\mathbf{v}}_j^\tau \mathbf{X}_i, \hat{\mathbf{v}}_k^\tau \mathbf{X}_i\right)&amp;, \quad i = 1,2, \dots, n  
\end{aligned}
\]</span></p>
<p>Since the first two principal components will capture the most variance &ndash; and hence the most useful information &ndash; of all possible pairs of principal components, we typically would set <span class="math inline">\(j = 1, k = 2\)</span> and plot the first two sample PC scores against each other. In <code>rrr</code> this is the default.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/pairwise_plot.html">pairwise_plot</a></span>(digits_features, digits_class, <span class="dt">type =</span> <span class="st">"pca"</span>)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-23-1.png" width="672"></p>
<p>We can set the <span class="math inline">\(x\)</span>- and <span class="math inline">\(y\)</span>-axes to whichever pairs of PC scores we would like to plot by changing the <code>pair_x</code> and <code>pair_y</code> arguments.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/pairwise_plot.html">pairwise_plot</a></span>(digits_features, digits_class, <span class="dt">type =</span> <span class="st">"pca"</span>, <span class="dt">pair_x =</span> <span class="dv">1</span>, <span class="dt">pair_y =</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-24-1.png" width="672"></p>
</div>
<div id="plot-all-pairs-of-pc-scores-with-allpairs_plot" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#plot-all-pairs-of-pc-scores-with-allpairs_plot" class="anchor"> </a></body></html>Plot all pairs of PC scores with <code><a href="../reference/allpairs_plot.html">allpairs_plot()</a></code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">args</span>(allpairs_plot)</code></pre></div>
<pre><code>## function (x, y, type = "pca", rank, k = 0) 
## NULL</code></pre>
<p>Alternatively, we can look at structure in the data by plotting all PC pairs, along with some other visual diagnostics with <code><a href="../reference/allpairs_plot.html">allpairs_plot()</a></code>. Along with plotting principal component scores against each other, the plot matrix also shows histograms and box plots to show how the points are distributed along principal component axes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/allpairs_plot.html">allpairs_plot</a></span>(digits_features, digits_class, <span class="dt">type =</span> <span class="st">"pca"</span>, <span class="dt">rank =</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-26-1.png" width="672"></p>
</div>
<div id="fitting-a-pca-model" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#fitting-a-pca-model" class="anchor"> </a></body></html>Fitting a PCA Model</h3>
</div>
<div id="fit-model-with-rrr" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#fit-model-with-rrr" class="anchor"> </a></body></html>Fit model with <code><a href="../reference/rrr.html">rrr()</a></code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/rrr.html">rrr</a></span>(digits_features, digits_features, <span class="dt">type =</span> <span class="st">"pca"</span>, <span class="dt">rank  =</span> <span class="dv">3</span>)</code></pre></div>
<pre><code>## $means
##        V1        V2        V3        V4        V5        V6        V7 
## 38.814320 85.120269 40.605622 83.774199 49.770378 65.573144 51.220251 
##        V8        V9       V10       V11       V12       V13       V14 
## 44.498999 56.868541 33.695961 60.516376 34.826510 55.022289 34.937045 
##       V15       V16       V17       V18       V19       V20       V21 
## 47.287482 28.845342  4.431587  5.341794 38.814320 85.120269 40.605622 
##       V22       V23       V24       V25       V26       V27       V28 
## 83.774199 49.770378 65.573144 51.220251 44.498999 56.868541 33.695961 
##       V29       V30       V31       V32       V33       V34 
## 60.516376 34.826510 55.022289 34.937045 47.287482 28.845342 
## 
## $C
## # A tibble: 34 &times; 34
##             V1           V2           V3           V4           V5
##          &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;
## 1   0.15227028  0.041971586  0.057621563  0.032824603 -0.083070856
## 2   0.04197159  0.013118640  0.010598715  0.005435764 -0.027861499
## 3   0.05762156  0.010598715  0.057059768  0.042972135  0.005136858
## 4   0.03282460  0.005435764  0.042972135  0.034784478  0.014445469
## 5  -0.08307086 -0.027861499  0.005136858  0.014445469  0.083611542
## 6  -0.00509760 -0.010118470  0.048612391  0.041246125  0.054433598
## 7  -0.12925579 -0.039313730 -0.047145641 -0.030698476  0.070011543
## 8  -0.03613352 -0.021185046  0.025475341  0.019298193  0.056664934
## 9  -0.04375152 -0.018001465 -0.036555354 -0.038171115 -0.002983700
## 10 -0.05163631 -0.020586646 -0.022911782 -0.022807068  0.019987392
## # ... with 24 more rows, and 29 more variables: V6 &lt;dbl&gt;, V7 &lt;dbl&gt;,
## #   V8 &lt;dbl&gt;, V9 &lt;dbl&gt;, V10 &lt;dbl&gt;, V11 &lt;dbl&gt;, V12 &lt;dbl&gt;, V13 &lt;dbl&gt;,
## #   V14 &lt;dbl&gt;, V15 &lt;dbl&gt;, V16 &lt;dbl&gt;, V17 &lt;dbl&gt;, V18 &lt;dbl&gt;, V19 &lt;dbl&gt;,
## #   V20 &lt;dbl&gt;, V21 &lt;dbl&gt;, V22 &lt;dbl&gt;, V23 &lt;dbl&gt;, V24 &lt;dbl&gt;, V25 &lt;dbl&gt;,
## #   V26 &lt;dbl&gt;, V27 &lt;dbl&gt;, V28 &lt;dbl&gt;, V29 &lt;dbl&gt;, V30 &lt;dbl&gt;, V31 &lt;dbl&gt;,
## #   V32 &lt;dbl&gt;, V33 &lt;dbl&gt;, V34 &lt;dbl&gt;
## 
## $PC
## # A tibble: 34 &times; 3
##            PC1          PC2         PC3
##          &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;
## 1   0.02861158 -0.142768395 -0.36203431
## 2   0.04680878 -0.033512929 -0.09901748
## 3  -0.13901701  0.046687918 -0.18855842
## 4  -0.10267046  0.083217740 -0.13159811
## 5  -0.15980723  0.196663538  0.13927183
## 6  -0.24050660  0.117714673 -0.05134764
## 7  -0.10613246  0.031714447  0.33613210
## 8  -0.28960021 -0.002314316  0.07783244
## 9  -0.11746249 -0.263105908  0.21532196
## 10 -0.14441246 -0.150734547  0.19065755
## # ... with 24 more rows
## 
## $goodness_of_fit
##       PC1       PC2       PC3 
## 0.7168777 0.4680599 0.3144671</code></pre>
</div>
</div>
<div id="canonical-variate-analysis" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#canonical-variate-analysis" class="anchor"> </a></body></html>Canonical Variate Analysis</h2>
<div id="cva-as-a-special-case-of-reduced-rank-regression" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#cva-as-a-special-case-of-reduced-rank-regression" class="anchor"> </a></body></html>CVA as a Special Case of Reduced-Rank Regression</h3>
<p>Canonical Variate Analysis<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> is a method of linear dimensionality reduction that turns the original <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> into new variables <span class="math inline">\(\boldsymbol{\xi}\)</span> and <span class="math inline">\(\boldsymbol{\omega}\)</span>, respectively. Canonical variate analysis can be performed as a special case of reduced-rank regression.</p>
<p>Set <span class="math inline">\(\mathbf{\Gamma} = \mathbf{\Sigma}_{YY}^{-1}\)</span>. Then, the <span class="math inline">\(t\)</span> new pairs of canonical variables <span class="math inline">\(\left(\xi_i, \omega_i\right), i = 1, \dots, t\)</span> are calculated by fitting a reduced-rank &ndash; rank <span class="math inline">\(t\)</span> &ndash; regression equation. The canonical variate scores are given by</p>
<p><span class="math display">\[
\boldsymbol{\xi}^{\left(t\right)} = \mathbf{G}^{\left(t\right)}\mathbf{X}, \quad \boldsymbol{\omega}^{\left(t\right)} = \mathbf{H}^{\left(t\right)} \mathbf{Y},
\]</span></p>
<p>with</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{G}^{\left(t\right)} &amp; = \mathbf{B}^{\left(t\right)} \\
\mathbf{H}^{\left(t\right)} &amp; = \mathbf{A}^{\left(t\right)-} \\
  \end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{A}^{\left(t\right)}, \mathbf{B}^{\left(t\right)}\)</span> are the matrices from the reduced-rank regression formulation above.</p>
<p>Note that <span class="math inline">\(\mathbf{H}^{\left(t\right)} = \mathbf{A}^{\left(t\right)-}\)</span> is the generalized inverse of <span class="math inline">\(\mathbf{A}^{\left(t\right)}\)</span>. When <span class="math inline">\(t = s, \mathbf{H}^{\left(s\right)} = \mathbf{A}^{\left(t\right)+}\)</span> is the unique Moore-Penrose generalized inverse of <span class="math inline">\(\mathbf{A}^{\left(t\right)}\)</span>.</p>
</div>
</div>
<div id="the-combo17-data-set" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#the-combo17-data-set" class="anchor"> </a></body></html>The <code>COMBO17</code> Data Set</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### COMBO-17 galaxy data
<span class="kw">data</span>(COMBO17)
galaxy &lt;-<span class="st"> </span><span class="kw">as_data_frame</span>(COMBO17) %&gt;%
<span class="st">       </span><span class="kw">select</span>(-<span class="kw">starts_with</span>(<span class="st">"e."</span>), -Nr, -UFS:-IFD) %&gt;%
<span class="st">       </span><span class="kw">na.omit</span>()</code></pre></div>
<p>This data set<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> comes from a public catalogue of objects in the Chandra Deep Field South, an area of the sky. This subset of the catalogue is all the objects classified as &ldquo;Galaxies&rdquo;, with only observations that do not have any missing values.<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">galaxy_x &lt;-<span class="st"> </span>galaxy %&gt;%
<span class="st">  </span><span class="kw">select</span>(-Rmag:-chi2red)

galaxy_y &lt;-<span class="st"> </span>galaxy %&gt;%
<span class="st">  </span><span class="kw">select</span>(Rmag:chi2red)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GGally::<span class="kw">ggcorr</span>(galaxy_x)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-30-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GGally::<span class="kw">ggcorr</span>(galaxy_y)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-31-1.png" width="672"></p>
<p>We can see above the heavy correlation among the <span class="math inline">\(X_i\)</span>s and among the <span class="math inline">\(Y_i\)</span>s. This data set, therefore, makes for a good candidate to perform canonical variate analysis.</p>
<div id="assessing-effective-dimensionality" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#assessing-effective-dimensionality" class="anchor"> </a></body></html>Assessing Effective Dimensionality</h3>
<p>Estimate <span class="math inline">\(t\)</span> and <span class="math inline">\(k\)</span> with <code><a href="../reference/rank_trace.html">rank_trace()</a></code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/rank_trace.html">rank_trace</a></span>(galaxy_x, galaxy_y, <span class="dt">type =</span> <span class="st">"cva"</span>)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-32-1.png" width="672"></p>
</div>
<div id="diagnostics-1" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#diagnostics-1" class="anchor"> </a></body></html>Diagnostics</h3>
<p>Plot and print residuals with <code><a href="../reference/residuals.html">residuals()</a></code>, setting <code>type = "cva"</code>.</p>
</div>
<div id="plot-residuals-1" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#plot-residuals-1" class="anchor"> </a></body></html>Plot Residuals</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/residuals.html">residuals</a></span>(galaxy_x, galaxy_y, <span class="dt">type =</span> <span class="st">"cva"</span>, <span class="dt">rank =</span> <span class="dv">2</span>, <span class="dt">k =</span> <span class="fl">0.001</span>)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-33-1.png" width="672"></p>
</div>
<div id="print-residuals" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#print-residuals" class="anchor"> </a></body></html>Print Residuals</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/residuals.html">residuals</a></span>(galaxy_x, galaxy_y, <span class="dt">type =</span> <span class="st">"cva"</span>, <span class="dt">rank =</span> <span class="dv">2</span>, <span class="dt">k =</span> <span class="fl">0.001</span>, <span class="dt">plot =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## # A tibble: 3,462 &times; 3
##    index        CV1        CV2
##    &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;
## 1      1 -0.3202596  0.5609487
## 2      2 -0.3373066 -0.3235156
## 3      3  0.2342563  0.2243338
## 4      4  0.2188205  0.6709930
## 5      5 -0.7183237  1.5344605
## 6      6 -1.0225019  0.1872628
## 7      7  0.2784014  0.5923622
## 8      8 -0.1242796  0.6893633
## 9      9  0.2544044  0.2704908
## 10    10 -0.6856947 -0.1686793
## # ... with 3,452 more rows</code></pre>
</div>
<div id="plot-pairwise-canonical-variate-scores" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#plot-pairwise-canonical-variate-scores" class="anchor"> </a></body></html>Plot Pairwise Canonical Variate Scores</h3>
<p>Plot canonical variate scores with <code><a href="../reference/pairwise_plot.html">pairwise_plot()</a></code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/pairwise_plot.html">pairwise_plot</a></span>(galaxy_x, galaxy_y, <span class="dt">type =</span> <span class="st">"cva"</span>, <span class="dt">pair_x =</span> <span class="dv">1</span>, <span class="dt">k =</span> <span class="fl">0.001</span>)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-35-1.png" width="672"></p>
<p>Choose which pair of canonical variate scores to plot by changing the argument <code>pair_x</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/pairwise_plot.html">pairwise_plot</a></span>(galaxy_x, galaxy_y, <span class="dt">type =</span> <span class="st">"cva"</span>, <span class="dt">pair_x =</span> <span class="dv">6</span>)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-36-1.png" width="672"></p>
</div>
<div id="fit-reduced-rank-canonical-variate-model" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#fit-reduced-rank-canonical-variate-model" class="anchor"> </a></body></html>Fit Reduced-Rank Canonical Variate Model</h3>
<p>Fit model with <code><a href="../reference/rrr.html">rrr()</a></code>, setting <code>type = "cva"</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/rrr.html">rrr</a></span>(galaxy_x, galaxy_y, <span class="dt">type =</span> <span class="st">"cva"</span>, <span class="dt">rank =</span> <span class="dv">2</span>, <span class="dt">k =</span> <span class="fl">0.001</span>)</code></pre></div>
<pre><code>## $mean
##              [,1]
## Rmag    28.523043
## ApDRmag  1.130048
## mumax   26.803027
## Mcz     -1.194108
## MCzml   -1.050103
## chi2red  2.088291
## 
## $G
##           UjMAG       BjMAG      VjMAG     usMAG      gsMAG      rsMAG
## [1,]  0.1315681 0.003160348  0.3435075 0.1039602 0.06062877 -0.2369377
## [2,] -0.9238507 0.030925869 -2.0418261 1.2086276 0.38691864  1.9345899
##           UbMAG       BbMAG       VnMAG     S280MAG     W420FE     W462FE
## [1,]  0.1732176 -0.11817306  0.01467586 0.006356947 -0.4377527 -0.4795137
## [2,] -0.6174437  0.01606138 -0.07708084 0.039012047 -3.9767464 -2.3909886
##          W485FD     W518FE     W571FS     W604FE     W646FD     W696FE
## [1,] -0.4020248 -0.2363471 -0.1188914 0.04050373  0.1684356  0.1456478
## [2,] -1.3831963  0.6639545 -0.2649454 0.42582774 -0.1510068 -0.8822436
##          W753FE     W815FS    W856FD     W914FD      W914FE
## [1,] 0.29307078  0.0239559 0.0266074 -0.2535550 -0.06707906
## [2,] 0.08978215 -1.7740911 0.2191533  0.5857208  0.77233519
## 
## $H
##           Rmag     ApDRmag      mumax        Mcz      MCzml    chi2red
## [1,] 0.5596114  0.47045788 -0.1373677 -1.4375179 -1.3869182  0.5791827
## [2,] 0.2922679 -0.09798891  0.4366839  0.5862785  0.5703691 -0.2163637
## 
## $canonical_corr
## [1] 0.927311439 0.354708001 0.063114244 0.018542243 0.005976463 0.003749350</code></pre>
</div>
<div id="print-canonical-variate-scores" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#print-canonical-variate-scores" class="anchor"> </a></body></html>Print Canonical Variate scores</h3>
<p>Print canonical variate scores with <code><a href="../reference/scores.html">scores()</a></code>, setting <code>type = "cva"</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/scores.html">scores</a></span>(galaxy_x, galaxy_y, <span class="dt">type =</span> <span class="st">"cva"</span>, <span class="dt">rank =</span> <span class="dv">2</span>, <span class="dt">k =</span> <span class="fl">0.001</span>)</code></pre></div>
<pre><code>## $xi
## # A tibble: 3,462 &times; 2
##            xi1         xi2
##          &lt;dbl&gt;       &lt;dbl&gt;
## 1   0.10836091  0.18543845
## 2  -0.04389050  1.45530270
## 3  -0.93827110  0.05929021
## 4   0.05123639  0.23079948
## 5   0.04861913 -0.28347051
## 6  -0.57153625  0.37372222
## 7   0.44724681  0.48578465
## 8   0.52712428  0.22804914
## 9   0.11200447  0.17947745
## 10  3.05322631 -0.25235663
## # ... with 3,452 more rows
## 
## $omega
## # A tibble: 3,462 &times; 2
##        omega1     omega2
##         &lt;dbl&gt;      &lt;dbl&gt;
## 1  -0.2118987  0.7463871
## 2  -0.3811971  1.1317871
## 3  -0.7040148  0.2836240
## 4   0.2700569  0.9017924
## 5  -0.6697045  1.2509899
## 6  -1.5940381  0.5609850
## 7   0.7256482  1.0781469
## 8   0.4028447  0.9174124
## 9   0.3664088  0.4499682
## 10  2.3675316 -0.4210359
## # ... with 3,452 more rows
## 
## $canonical_corr
## [1] 0.927311439 0.354708001 0.063114244 0.018542243 0.005976463 0.003749350</code></pre>
</div>
</div>
<div id="linear-discriminant-analysis" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#linear-discriminant-analysis" class="anchor"> </a></body></html>Linear Discriminant Analysis</h2>
<div id="lda-as-a-special-case-of-cva" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#lda-as-a-special-case-of-cva" class="anchor"> </a></body></html>LDA as a Special Case of CVA</h3>
<p>Linear discriminant analysis is a classification procedure. We can turn it into a regression procedure &ndash; specifically a reduced-rank canonical variate procedure &ndash; in the following way.</p>
<p>Let each <span class="math inline">\(i = 1, 2, \dots, n\)</span> observation belong to one, and only one, of <span class="math inline">\(K = s + 1\)</span> distinct classes.</p>
<p>We can construct an <em>indicator response matrix</em>, <span class="math inline">\(\mathbf{Y}\)</span> where each row <span class="math inline">\(i\)</span> is an indicator response vector for the <span class="math inline">\(i\)</span>th observation. The vector will have a 1 in the column that represents that class to which the observation belongs and will be 0 elsewhere.</p>
<p>We then regress this <span class="math inline">\(Y\)</span> binary response matrix against the matrix <span class="math inline">\(X\)</span> of predictor variables.</p>
<p>Linear discriminant analysis requires the assumptions that each class is normally distributed and that the covariance matrix of each class is equal to all others.</p>
<p>While these assumptions will not be met in all cases, when they are &ndash; and when the classes are well separated &ndash; linear discriminant analysis is a very efficient classification method.</p>
</div>
</div>
<div id="the-iris-data-set" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#the-iris-data-set" class="anchor"> </a></body></html>The <code>iris</code> Data Set</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(iris)
iris &lt;-<span class="st"> </span><span class="kw">as_data_frame</span>(iris)</code></pre></div>
<p>This is R.A. Fisher&rsquo;s classic <code>iris</code> data set that comes packaged in base <code>R</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_features &lt;-<span class="st"> </span>iris %&gt;%
<span class="st">  </span><span class="kw">select</span>(-Species)

iris_class &lt;-<span class="st"> </span>iris %&gt;%
<span class="st">  </span><span class="kw">select</span>(Species)</code></pre></div>
<div id="assesssing-effective-dimensionality" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#assesssing-effective-dimensionality" class="anchor"> </a></body></html>Assesssing Effective Dimensionality</h3>
<p>Assessing the rank <span class="math inline">\(t\)</span> of this reduced-rank regression is equivalent to determining the number of linear discriminant functions that best discriminate between the <span class="math inline">\(K\)</span> classes, with <span class="math inline">\(\mathrm{min}\left(r, s\right) = \mathrm{min}\left(r, K - 1\right)\)</span> maximum number of linear discriminant functions.</p>
<p>Generally, plotting linear discriminant functions against each other, i.e., the first and second linear discriminant functions, is used to determine whether sufficient discrimination is obtained.</p>
<p>Plotting techniques are discussed in the following section.</p>
</div>
<div id="plot-lda-pairs" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#plot-lda-pairs" class="anchor"> </a></body></html>Plot LDA Pairs</h3>
<p>Plot LDA pairs with <code><a href="../reference/pairwise_plot.html">pairwise_plot()</a></code>, setting <code>type = "lda"</code>.</p>
<p>A typical graphical display for multiclass LDA is to plot the <span class="math inline">\(j\)</span>th discriminant scores for the <span class="math inline">\(n\)</span> points against the <span class="math inline">\(k\)</span> discriminant scores.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/pairwise_plot.html">pairwise_plot</a></span>(iris_features, iris_class, <span class="dt">type =</span> <span class="st">"lda"</span>, <span class="dt">k =</span> <span class="fl">0.0001</span>)</code></pre></div>
<p><img src="rrr_files/figure-html/unnamed-chunk-41-1.png" width="672"></p>
</div>
<div id="fitting-lda-models" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#fitting-lda-models" class="anchor"> </a></body></html>Fitting LDA Models</h3>
<p>Fit LDA model with <code><a href="../reference/rrr.html">rrr()</a></code>, setting <code>type = "lda"</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/rrr.html">rrr</a></span>(iris_features, iris_class, <span class="dt">type =</span> <span class="st">"lda"</span>, <span class="dt">k =</span> <span class="fl">0.0001</span>)</code></pre></div>
<pre><code>## $G
## # A tibble: 4 &times; 2
##          LD1          LD2
##        &lt;dbl&gt;        &lt;dbl&gt;
## 1 -0.1427550  0.009711862
## 2 -0.2639139  0.905610891
## 3  0.3792278 -0.388369442
## 4  0.4826420  1.184645528
## 
## $H
## # A tibble: 2 &times; 2
##         LD1        LD2
##       &lt;dbl&gt;      &lt;dbl&gt;
## 1 -1.123703 -0.3319998
## 2 -0.265336 -1.1058423</code></pre>
</div>
<div id="print-lda-scores-with-scores" class="section level3">
<h3 class="hasAnchor"><html><body><a href="#print-lda-scores-with-scores" class="anchor"> </a></body></html>Print LDA Scores with <code><a href="../reference/scores.html">scores()</a></code></h3>
<p>Print linear discriminant scores and the class means with <code><a href="../reference/scores.html">scores()</a></code>, setting <code>type = "lda"</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/scores.html">scores</a></span>(iris_features, iris_class, <span class="dt">type =</span> <span class="st">"lda"</span>, <span class="dt">k =</span> <span class="fl">0.0001</span>)</code></pre></div>
<pre><code>## $scores
## # A tibble: 150 &times; 3
##          LD1          LD2  class
##        &lt;dbl&gt;        &lt;dbl&gt; &lt;fctr&gt;
## 1  -1.387251  0.125583983 setosa
## 2  -1.226743 -0.329163835 setosa
## 3  -1.288897 -0.111147085 setosa
## 4  -1.172385 -0.280353248 setosa
## 5  -1.399367  0.215173886 setosa
## 6  -1.325346  0.611160171 setosa
## 7  -1.241218  0.148631516 setosa
## 8  -1.308661 -0.004785237 setosa
## 9  -1.128974 -0.424580855 setosa
## 10 -1.263476 -0.395904243 setosa
## # ... with 140 more rows
## 
## $class_means
## # A tibble: 3 &times; 3
##          LD1        LD2      class
##        &lt;dbl&gt;      &lt;dbl&gt;     &lt;fctr&gt;
## 1 -1.3498742  0.4053488     setosa
## 2  0.3239739 -1.3716811 versicolor
## 3  1.0259003  0.9663322  virginica</code></pre>
</div>
</div>
<div class="footnotes">
<hr><ol><li id="fn1"><p>Hotelling, H. (1936). Analysis of a complex of statistical variables into principal components, <em>Journal of Educational Psychology</em>, <strong>24</strong>, 417-411, 498-520.<a href="#fnref1">&#8617;</a></p></li>
<li id="fn2"><p>Hotelling, H. (1936). Relations between two sets of variates, <em>Biometrika</em>, <strong>28</strong>, 321-377.<a href="#fnref2">&#8617;</a></p></li>
<li id="fn3"><p>Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems, <em>Annals of Eugencis</em>, <strong>7</strong>, 179-188.<a href="#fnref3">&#8617;</a></p></li>
<li id="fn4"><p>Izenman, A.J. (2008). <em>Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning.</em> Springer.<a href="#fnref4">&#8617;</a></p></li>
<li id="fn5"><p>Anderson, R.L and Bancroft, T.A (1952). <em>Statistical Theory in Research</em>, New York: McGraw-Hill. p.&nbsp;205.<a href="#fnref5">&#8617;</a></p></li>
<li id="fn6"><p>Hoerl, A.E. and Kennard, R. (1970). Ridge regression: Biased estimation for non-orthogonal problems. <em>Technometrics</em> <strong>12</strong>: 55-67. Reprinted in <em>Technometrics</em>, <strong>42</strong> (2000), 80-86.<a href="#fnref6">&#8617;</a></p></li>
<li id="fn7"><p>Aldrin, Magne. &ldquo;Multivariate Prediction Using Softly Shrunk Reduced-Rank Regression.&rdquo; The American Statistician 54.1 (2000): 29. Web.<a href="#fnref7">&#8617;</a></p></li>
<li id="fn8"><p>Izenman, A.J. (2008). <em>Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning.</em> Springer.<a href="#fnref8">&#8617;</a></p></li>
<li id="fn9"><p>Alimoglu, F. (1995). Combining multiple classifiers for pen-based handwritten digit recognition, M.Sc. thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University, Istanbul, Turkey.<a href="#fnref9">&#8617;</a></p></li>
<li id="fn10"><p>Hotelling, H. (1936). Relations between two sets of variates, <em>Biometrika</em>, <strong>28</strong>, 321-377.<a href="#fnref10">&#8617;</a></p></li>
<li id="fn11"><p>Wolf, C., Meisenheimer, M., Kleinheinrich, M., Borch, A., Dye, S., Gray, M., Wisotski, L., Bell, E.F., Rix, H.W., Cimatti, A., Hasinger, G., and Szokoly, G. (2004). A catalogue of the Chandra Deep Field South with multi-colour classification and photometric redshifts from COMBO-17, Astronomy &amp; Astrophysics, <a href="https://arxiv.org/pdf/astro-ph/0403666.pdf" class="uri">https://arxiv.org/pdf/astro-ph/0403666.pdf</a><a href="#fnref11">&#8617;</a></p></li>
<li id="fn12"><p>Donald Richards in the Department of Statistics at Pennsylvania State University helped Dr.&nbsp;Izenman in understanding the variables in this data set. The package author, by extension, would also like to acknowledge Dr.&nbsp;Richards for help in understanding the data set used in the text <em>Modern Multivariate Statistical Techniques</em>.<a href="#fnref12">&#8617;</a></p></li>
</ol></div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2>Contents</h2>
      <ul class="nav nav-pills nav-stacked"><li><a href="#classical-multivariate-regression">Classical Multivariate Regression</a></li>
      <li><a href="#the-tobacco-data-set">The <code>tobacco</code> Data Set</a></li>
      <li><a href="#reduced-rank-regression">Reduced-Rank Regression</a></li>
      <li><a href="#principal-components-analysis">Principal Components Analysis</a></li>
      <li><a href="#the-pendigits-data-set">The <code>pendigits</code> Data Set</a></li>
      <li><a href="#canonical-variate-analysis">Canonical Variate Analysis</a></li>
      <li><a href="#the-combo17-data-set">The <code>COMBO17</code> Data Set</a></li>
      <li><a href="#linear-discriminant-analysis">Linear Discriminant Analysis</a></li>
      <li><a href="#the-iris-data-set">The <code>iris</code> Data Set</a></li>
      </ul></div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Chris Addy.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer></div>

  </body></html>
