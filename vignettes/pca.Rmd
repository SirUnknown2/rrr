---
title: "rrr for Principal Component Analysis"
author: "Chris Addy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Mathematical Background

### PCA is a Special Case of [Reduced-Rank Regression](rrr.html)

Set

$$
\begin{aligned}
\mathbf{Y} & \equiv \mathbf{X} \\
\mathbf{\Gamma} & = \mathbf{I}_r
\end{aligned}
$$

Then, the least squares criterion

$$
\mathrm{E}\left[\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B} \mathbf{X}\right)\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B} \mathbf{X}\right)^\tau\right]
$$

is minimized when

$$
\begin{aligned}
  \mathbf{A}^{\left(t\right)} & = \left(\mathbf{v}_1, \dots, \mathbf{v}_t\right) \\
  \mathbf{B}^{\left(t\right)} & = \mathbf{A}^{\left(t\right) \tau} \\
  \boldsymbol{\mu}^{\left(t\right)} & = \left(\mathbf{I}_r - \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\right)\boldsymbol{\mu}_X \\
\end{aligned}
$$

where $\mathbf{v}_j = \mathbf{v}_j \left(\mathbf{\Sigma}_{XX}\right)$ is the eigenvector associated with the $j$th largest eigenvalue of  $\mathbf{\Sigma}_{XX}.$

The best reduced-rank approximation to the original $\mathbf{X}$ is

$$
\begin{aligned}
\hat{\mathbf{X}}^{\left(t\right)} & =
  \boldsymbol{\mu}^{\left(t\right)} + \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)} \mathbf{X} \\
  & \mathrm{or} \\
  \hat{\mathbf{X}} & = \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\mathbf{X}_c \\
\end{aligned}
$$

where $\mathbf{X}_c$ is the vector $mathbf{X}$ after mean-centering.

The first principle component is a latent variable that is a linear combination of the $X_i$s that maximizes the variance among the $X_i$s. The second principle component is another linear combination that maximizes the variance among the $X_i$s subject to being independent of the first principal component. There are $r$ possible principal components, each independent of each other, that capture decreasing amounts of variance. The goal is to use as few principle components as necessary to capture the variance in the data and reduce dimensionality. The question of how many principle components to keep is equivalent to assessing the effective dimensionality $t$ of the reduced-rank regression.

## The `pendigits` Data Set

```{r message = FALSE, warning = FALSE}
data(pendigits)
digits <- as_data_frame(pendigits) %>% select(-V36)

glimpse(digits)
```

```{r}
digits_features <- digits %>% select(-V35)
digits_class <- digits %>% select(V35)
```

```{r}
GGally::ggcorr(digits_features)
```

## Assessing Dimensionality

```{r}
pca_gof(digits_features) %>% round(4)
```

### Estimate $t$ and ridge constant $k$ with `pca_rank_trace()`

```{r}
args(pca_rank_trace)
``` 

`pca_rank_trace()` is a special case of `rank_trace()`, with $\mathbf{Y} \equiv \mathbf{X}$ and $\mathbf{\Gamma} = \mathbf{I}_r$.

```{r}
pca_rank_trace(digits_features)
```

```{r}
pca_rank_trace(digits_features, plot = FALSE)
```

## Plot Principal Component Scores 

### Pairwise Plots with `pca_pairwise_plot()`

```{r}
args(pca_pairwise_plot)
```

A common PCA method of visualization for diagnostic and analysis purposes is to plot the $j$th sample PC scores against the $k$th PC scores,

$$
\begin{aligned}
  \left(\xi_{ij}, \xi_{ik}\right) & \\
  = \left(\hat{\mathbf{v}}_j^\tau \mathbf{X}_i, \hat{\mathbf{v}}_k^\tau \mathbf{X}_i\right)&, \quad i = 1,2, \dots, n  
\end{aligned}
$$

Since the first two principal components will capture the most variance -- and hence the most useful information -- of all possible pairs of principal components, we typically would set $j = 1, k = 2$ and plot the first two sample PC scores against each other. In `rrr` this is the default.

```{r}
pca_pairwise_plot(digits_features, class_labels = digits_class)
```

We can set the $x$- and $y$-axes to whichever pairs of PC scores we would like to plot by changing the `pc_x` and `pc_y` arguments.

```{r}
pca_pairwise_plot(digits_features, pc_x = 1, pc_y = 3, class_labels = digits_class)
```

### Plot all pairs of PC scores with `pca_allpairs_plot()`

```{r}
#args(pca_allpairs_plot)
```

```{r}
#pca_allpairs_plot(digits_features, rank = 3, class_labels = digits_class)
```

## Fitting a PCA Model

### Fit model with `pca()`

```{r}
args(pca)
```

```{r}
digits_class <- digits %>% dplyr::select(V35)
digits_features <- digits %>% dplyr::select(-V5, -V6)

digits_pca <- pca(digits_features, rank  = 3)

digits_pca
```

```{r}
GGally::ggcorr(digits_pca)
```