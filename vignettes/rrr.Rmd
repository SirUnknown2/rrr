---
title: "`rrr` for Multivariate Analysis"
author: "Chris Addy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r message = FALSE, out.width = .8}
devtools::install_github("chrisaddy/rrr")
library(rrr)
#library(ggplot2)
library(dplyr)
```

### Reduced-Rank Regression and Its Special Cases

The multivariate linear regression model is given by

$$
\overset{s \times 1}{\mathbf{Y}} = \overset{s \times 1}{\boldsymbol{\mu}} + \overset{s \times r}{\mathbf{C}} \; \overset{r \times 1}{\mathbf{X}} + \overset{s \times 1}{\varepsilon}
$$

with

$$
\mathrm{E}\left(\varepsilon\right) = \mathbf{0}, \quad \mathrm{cov}\left(\varepsilon\right) = \mathbf{\Sigma}_{\varepsilon \varepsilon}
$$

and $\varepsilon$ is distributed independently of $\mathbf{X}.$

We introduce the possibility that $\mathbf{C}$ is rank-deficient, i.e.,

$$
\mathrm{rank}\left(\mathbf{C}\right) = t \quad \mathrm{min}\left(r, s \right)
$$

When $t = s$, the regression model is *full-rank*, and can be fit using multiple regression on each $Y_i \in \mathbf{Y}.$ When $t < s$, $\mathbf{C}$ can be decomposed into non-unique matrices $\mathbf{A}_{s \times t}$ and $\mathbf{B}_{t \times r}$, such that $\mathbf{C} = \mathbf{AB},$ and the multivariate regression model is given by

$$
\overset{s \times 1}{\mathbf{Y}} = \overset{s \times 1}{\boldsymbol{\mu}} + \overset{s \times t}{\mathbf{A}} \; \overset{t \times r}{\mathbf{B}} \; \overset{r \times 1}{\mathbf{X}} + \overset{s \times 1}{\varepsilon}
$$

Finding $\mathbf{A}, \mathbf{B}$, and ultimately the *reduced-rank regression coefficient* $\mathbf{C}^{\left(t\right)}$, is done by minimizing the weighted sum-of-squares criterion

$$
\mathrm{E}\left[\left(\mathbf{Y} - \boldsymbol{\mu} - \mathbf{ABX}\right)^\tau \mathbf{\Gamma}\left(\mathbf{Y} - \boldsymbol{\mu} - \mathbf{ABX}\right)\right]
$$

where $\boldsymbol{\Gamma}$ is a positive-definite symmetric $\left(s \times s\right)$-matrix of weights, the expectation of which is taken over the joint distribution $\left(\mathbf{X}^\tau, \mathbf{Y}^\tau\right)^\tau$. This weighted sum-of-squares criterion is minimized when

$$
\begin{aligned}
	\boldsymbol{\mu}^\left(t\right) & = \boldsymbol{\mu}_Y - \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\boldsymbol{\mu}_X \\
	\mathbf{A}^{\left(t\right)} & = \mathbf{\Gamma}^{-1/2}\mathbf{V}_t \\
	\mathbf{B}^{\left(t\right)} & = \mathbf{V}_t^\tau \boldsymbol{\Gamma}^{-1/2}\mathbf{\Sigma}_{YX}\mathbf{XX}^{-1} \\
\end{aligned}
$$

where $\mathbf{V}_t = \left(\mathbf{v}_1, \dots, \mathbf{v}_t\right)$ is an $\left(s \times t\right)$-matrix, with $\mathbf{v}_j$ the eigenvector associated with the $j$th largest eigenvalue of

$$
\mathbf{\Gamma}^{1/2}\mathbf{\Sigma}_{YX} \mathbf{\Sigma}_{XX}^{-1} \mathbf{\Sigma}_{XY} \mathbf{\Gamma}^{1/2}
$$

Using different values of $\mathbf{\Gamma}$ we can use the reduced-rank regression model carry out classical multivariate procedures.

## Principal Components Analysis

Set

$$
\begin{aligned}
	\mathbf{Y} & \equiv \mathbf{X} \\
	s & = r \\
	\mathbf{\Gamma} & = \mathbf{I}_r \\
\end{aligned}
$$
	
Then, the least-squares error criterion

$$
	\mathrm{E}\left[\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B}\mathbf{X}\right)^\tau\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B}\mathbf{X}\right)\right]
$$

is minimized by the reduced-rank regression solution,

\begin{equation}
	\begin{aligned}
		\mathbf{A}^{\left(t\right)} & = \left(\mathbf{v}_1, \dots, \mathbf{v}_t\right) \\
		\mathbf{B}^{\left(t\right)} & = \mathbf{A}^{\left(t\right) \tau} \\
		\boldsymbol{\mu}^{\left(t\right)} & = \left(\mathbf{I}_r - \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\right)\boldsymbol{\mu}_X \\
	\end{aligned}
\end{equation}

where $\mathbf{v}_j = \mathbf{v}_j \left(\mathbf{\Sigma}_{XX}\right)$ is eigenvector associated with the $j$th largest eigenvalue of  $\mathbf{\Sigma}_{XX}.$

The best reduced-rank approximation to the original $\mathbf{X}$ is

\begin{equation}
	\begin{aligned}
		\hat{\mathbf{X}}^{\left(t\right)} & =
		\boldsymbol{\mu}^{\left(t\right)} + \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)} \mathbf{X}
	\end{aligned}
\end{equation}

```{r message = FALSE}
#require(readr)
#digits <- read_delim("http://astro.ocis.temple.edu/~alan/pendigits.txt", delim = " ", col_names = FALSE)

#digits_class <- digits %>% dplyr::select(X35)
#digits_features <- digits %>% dplyr::select(-X35, -X36)

#digits_pca <- rrpca(digits_features)

#pca_gof(digits_features) %>% round(4)

#pca_rank_trace(digits_features)

#pca_rank_trace_plot(digits_features)

#pc_pairwise_plot(digits_features, class_labels = digits_class)

#pc_plot_3D(digits_features, class_labels = digits_class)
```

## Canonical Variate Analysis

$\mathbf{X}, \mathbf{Y}$ are jointly distributed, with

\begin{equation}
	\begin{aligned}
		\mathrm{E}\left\{
		\begin{pmatrix}
			\mathbf{X} \\
			\mathbf{Y} \\
		\end{pmatrix}
		\right\} =
		\begin{pmatrix}
			\boldsymbol{\mu}_X \\
			\boldsymbol{\mu}_Y \\
		\end{pmatrix}, \quad
		\mathrm{cov}\left\{
		\begin{pmatrix}
			\mathbf{X} \\
			\mathbf{Y} \\
		\end{pmatrix}\right\} =
		\begin{pmatrix}
			\mathbf{\Sigma}_{XX} & \mathbf{\Sigma}_{XY} \\
			\mathbf{\Sigma}_{YX} & \mathbf{\Sigma}_{YY} \\
		\end{pmatrix}
	\end{aligned}
\end{equation}

Let $1 \leq t \leq \mathrm{min}\left(r, s\right)$. $\mathbf{X}$ and $\mathbf{Y}$ are linearly projected into new vector variates,

\begin{equation}
	\begin{aligned}
		\boldsymbol{\xi}_{t \times 1} = \mathbf{G}_{t \times r} \mathbf{X}_{r \times 1}, \quad \boldsymbol{\omega}_{t \times 1} = \mathbf{H}_{t \times s} \mathbf{X}_{s \times 1}
		\end{aligned}
		\end{equation}

Finding a solution to the problem

\begin{equation}
	\begin{aligned}
		\boldsymbol{\xi} \approx \boldsymbol{\nu} + \boldsymbol{\omega}
	\end{aligned}
\end{equation}

requires minimizing the least-squares criterion

$$
	\begin{aligned}
		\mathrm{E}\left[\left(\mathbf{H}\mathbf{Y} - \boldsymbol{\nu} - \mathbf{G}\mathbf{X}\right)\left(\mathbf{H}\mathbf{Y} - \boldsymbol{\nu} - \mathbf{G}\mathbf{X}\right)^\tau\right]
	\end{aligned},
$$

where we assume $\mathrm{cov}\left(\boldsymbol{\omega}\right) = \mathbf{I}_t$. This least-squares criterion is minimized when

\begin{equation}
	\begin{aligned}
		\boldsymbol{\nu}^{\left(t\right)} & = \mathbf{H}^{\left(t\right)} \boldsymbol{\mu}_Y - \mathbf{G}^{\left(t\right)}\boldsymbol{\mu}_Y \\
		\mathbf{G}^{\left(t\right)} & =
		\begin{pmatrix}
			\mathbf{v}_1^\tau \\
			\vdots \\
			\mathbf{v}_t^\tau \\
		\end{pmatrix}
		\mathbf{\Sigma}_{YY}^{-1/2} \mathbf{\Sigma}_{YX}\mathbf{\Sigma}_{XX}^{-1} \\
		\mathbf{H}^{\left(t\right)} & =
		\begin{pmatrix}
			\mathbf{v}_1^\tau \\
			\vdots \\
			\mathbf{v}_t^\tau \\
		\end{pmatrix}
		\mathbf{\Sigma}_{YY}^{-1/2} \\
	\end{aligned}
\end{equation}

where $\mathbf{v}_j$ is the eigenvector associated with the $j$th largest eigenvalue of

\begin{equation}
	\begin{aligned}
		\mathbf{R} & = \mathbf{\Sigma}_{YY}^{-1/2}\mathbf{\Sigma}_{YX} \mathbf{\Sigma}_{XX}^{-1} \mathbf{\Sigma}_{XY} \mathbf{\Sigma}_{YY}^{-1/2}
	\end{aligned}
\end{equation}

```{r}
### COMBO-17 galaxy data
#data(COMBO17)
#galaxy <- as_data_frame(COMBO17) %>%
#       select(-starts_with("e."), -Nr, -UFS:-IFD) %>%
#       na.omit()

#galaxy_x <- galaxy %>% select(-Rmag:-chi2red)
#galaxy_y <- galaxy %>% select(Rmag:chi2red)

#galaxy_x
#galaxy_y

#rrcva(galaxy_x, galaxy_y, rank = 2)
```

## Linear Discriminant Analysis

