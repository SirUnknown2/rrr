---
title: "Mathematical Background of Reduced-Rank Regression and its Special Cases"
author: "Chris Addy" 
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Classical Multivariate Regression

The multivariate regression model is given by

$$
\overset{s \times 1}{\mathbf{Y}} = \overset{s \times 1}{\boldsymbol{\mu}} + \overset{s \times r}{\mathbf{C}} \; \overset{r \times 1}{\mathbf{X}} + \overset{s \times 1}{\varepsilon}
$$

with

$$
\mathrm{E}\left(\varepsilon\right) = \mathbf{0}, \quad \mathrm{cov}\left(\varepsilon\right) = \mathbf{\Sigma}_{\varepsilon \varepsilon}
$$

and $\varepsilon$ is distributed independently of $\mathbf{X}.$

## Reduced-Rank Regression

We introduce the possibility that $\mathbf{C}$ is rank-deficient, i.e.,

$$
\mathrm{rank}\left(\mathbf{C}\right) = t \quad \mathrm{min}\left(r, s \right)
$$

When $t = s$, the regression model is *full-rank*, and can be fit using multiple regression on each $Y_i \in \mathbf{Y}.$ When $t < s$, $\mathbf{C}$ can be decomposed into non-unique matrices $\mathbf{A}_{s \times t}$ and $\mathbf{B}_{t \times r}$, such that $\mathbf{C} = \mathbf{AB},$ and the multivariate regression model is given by

$$
\overset{s \times 1}{\mathbf{Y}} = \overset{s \times 1}{\boldsymbol{\mu}} + \overset{s \times t}{\mathbf{A}} \; \overset{t \times r}{\mathbf{B}} \; \overset{r \times 1}{\mathbf{X}} + \overset{s \times 1}{\varepsilon}
$$

Finding $\mathbf{A}, \mathbf{B}$, and ultimately the *reduced-rank regression coefficient* $\mathbf{C}^{\left(t\right)}$, is done by minimizing the weighted sum-of-squares criterion

$$
\mathrm{E}\left[\left(\mathbf{Y} - \boldsymbol{\mu} - \mathbf{ABX}\right)^\tau \mathbf{\Gamma}\left(\mathbf{Y} - \boldsymbol{\mu} - \mathbf{ABX}\right)\right]
$$

where $\boldsymbol{\Gamma}$ is a positive-definite symmetric $\left(s \times s\right)$-matrix of weights, the expectation of which is taken over the joint distribution $\left(\mathbf{X}^\tau, \mathbf{Y}^\tau\right)^\tau$. This weighted sum-of-squares criterion is minimized when

$$
\begin{aligned}
\boldsymbol{\mu}^\left(t\right) & = \boldsymbol{\mu}_Y - \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\boldsymbol{\mu}_X \\
        \mathbf{A}^{\left(t\right)} & = \mathbf{\Gamma}^{-1/2}\mathbf{V}_t \\
        \mathbf{B}^{\left(t\right)} & = \mathbf{V}_t^\tau \boldsymbol{\Gamma}^{-1/2}\mathbf{\Sigma}_{YX}\mathbf{XX}^{-1} \\
\end{aligned}
$$

where $\mathbf{V}_t = \left(\mathbf{v}_1, \dots, \mathbf{v}_t\right)$ is an $\left(s \times t\right)$-matrix, with $\mathbf{v}_j$ the eigenvector associated with the $j$th largest eigenvalue of

$$
\mathbf{\Gamma}^{1/2}\mathbf{\Sigma}_{YX} \mathbf{\Sigma}_{XX}^{-1} \mathbf{\Sigma}_{XY} \mathbf{\Gamma}^{1/2}
$$

Using different values of $\mathbf{\Gamma}$ we can use the reduced-rank regression model carry out classical multivariate procedures.

## Principal Components Analysis

Set

$$
\begin{aligned}
        \mathbf{Y} & \equiv \mathbf{X} \\
        s & = r \\
        \mathbf{\Gamma} & = \mathbf{I}_r \\
\end{aligned}
$$

Then, the least-squares error criterion

$$
        \mathrm{E}\left[\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B}\mathbf{X}\right)^\tau\left(\mathbf{X} - \boldsymbol{\mu} - \mathbf{A}\mathbf{B}\mathbf{X}\right)\right]
$$

is minimized by the reduced-rank regression solution,

$$
                \mathbf{A}^{\left(t\right)} & = \left(\mathbf{v}_1, \dots, \mathbf{v}_t\right) \\
                \mathbf{B}^{\left(t\right)} & = \mathbf{A}^{\left(t\right) \tau} \\
                \boldsymbol{\mu}^{\left(t\right)} & = \left(\mathbf{I}_r - \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\right)\boldsymbol{\mu}_X \\
$$

where $\mathbf{v}_j = \mathbf{v}_j \left(\mathbf{\Sigma}_{XX}\right)$ is eigenvector associated with the $j$th largest eigenvalue of  $\mathbf{\Sigma}_{XX}.$

The best reduced-rank approximation to the original $\mathbf{X}$ is

$$
\hat{\mathbf{X}}^{\left(t\right)} & =
                \boldsymbol{\mu}^{\left(t\right)} + \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)} \mathbf{X}
$$

$\mathbf{X}, \mathbf{Y}$ are jointly distributed, with

$$
                \mathrm{E}\left\{
                \begin{pmatrix}
                        \mathbf{X} \\
                        \mathbf{Y} \\
                \end{pmatrix}
                \right\} =
                \begin{pmatrix}
                        \boldsymbol{\mu}_X \\
                        \boldsymbol{\mu}_Y \\
                \end{pmatrix}, \quad
                \mathrm{cov}\left\{
                \begin{pmatrix}
                        \mathbf{X} \\
                        \mathbf{Y} \\
                \end{pmatrix}\right\} =
                \begin{pmatrix}
                        \mathbf{\Sigma}_{XX} & \mathbf{\Sigma}_{XY} \\
                        \mathbf{\Sigma}_{YX} & \mathbf{\Sigma}_{YY} \\
                \end{pmatrix}
$$

Let $1 \leq t \leq \mathrm{min}\left(r, s\right)$. $\mathbf{X}$ and $\mathbf{Y}$ are linearly projected into new vector variates,

$$
                \boldsymbol{\xi}_{t \times 1} = \mathbf{G}_{t \times r} \mathbf{X}_{r \times 1}, \quad \boldsymbol{\omega}_{t \times 1} = \mathbf{H}_{t \times s} \mathbf{X}_{s \times 1}
$$

Finding a solution to the problem

$$
                \boldsymbol{\xi} \approx \boldsymbol{\nu} + \boldsymbol{\omega}
$$


requires minimizing the least-squares criterion

$$
        \begin{aligned}
                \mathrm{E}\left[\left(\mathbf{H}\mathbf{Y} - \boldsymbol{\nu} - \mathbf{G}\mathbf{X}\right)\left(\mathbf{H}\mathbf{Y} - \boldsymbol{\nu} - \mathbf{G}\mathbf{X}\right)^\tau\right]
        \end{aligned},
$$

where we assume $\mathrm{cov}\left(\boldsymbol{\omega}\right) = \mathbf{I}_t$. This least-squares criterion is minimized when

$$
                \boldsymbol{\nu}^{\left(t\right)} & = \mathbf{H}^{\left(t\right)} \boldsymbol{\mu}_Y - \mathbf{G}^{\left(t\right)}\boldsymbol{\mu}_Y \\
                \mathbf{G}^{\left(t\right)} & =
                \begin{pmatrix}
                        \mathbf{v}_1^\tau \\
                        \vdots \\
                        \mathbf{v}_t^\tau \\
                \end{pmatrix}
                \mathbf{\Sigma}_{YY}^{-1/2} \mathbf{\Sigma}_{YX}\mathbf{\Sigma}_{XX}^{-1} \\
                \mathbf{H}^{\left(t\right)} & =
                \begin{pmatrix}

 \mathbf{v}_1^\tau \\
                        \vdots \\
                        \mathbf{v}_t^\tau \\
                \end{pmatrix}
                \mathbf{\Sigma}_{YY}^{-1/2} \\
$$

where $\mathbf{v}_j$ is the eigenvector associated with the $j$th largest eigenvalue of

$$
                \mathbf{R} & = \mathbf{\Sigma}_{YY}^{-1/2}\mathbf{\Sigma}_{YX} \mathbf{\Sigma}_{XX}^{-1} \mathbf{\Sigma}_{XY} \mathbf{\Sigma}_{YY}^{-1/2}

B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
$$
