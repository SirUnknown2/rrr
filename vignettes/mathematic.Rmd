---
title: "Mathematical Background of Reduced-Rank Regression and its Special Cases"
author: "Chris Addy" 
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

$$
                \mathbf{A}^{\left(t\right)} & = \left(\mathbf{v}_1, \dots, \mathbf{v}_t\right) \\
                \mathbf{B}^{\left(t\right)} & = \mathbf{A}^{\left(t\right) \tau} \\
                \boldsymbol{\mu}^{\left(t\right)} & = \left(\mathbf{I}_r - \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)}\right)\boldsymbol{\mu}_X \\
$$

where $\mathbf{v}_j = \mathbf{v}_j \left(\mathbf{\Sigma}_{XX}\right)$ is eigenvector associated with the $j$th largest eigenvalue of  $\mathbf{\Sigma}_{XX}.$

The best reduced-rank approximation to the original $\mathbf{X}$ is

$$
\hat{\mathbf{X}}^{\left(t\right)} & =
                \boldsymbol{\mu}^{\left(t\right)} + \mathbf{A}^{\left(t\right)}\mathbf{B}^{\left(t\right)} \mathbf{X}
$$

$\mathbf{X}, \mathbf{Y}$ are jointly distributed, with

$$
                \mathrm{E}\left\{
                \begin{pmatrix}
                        \mathbf{X} \\
                        \mathbf{Y} \\
                \end{pmatrix}
                \right\} =
                \begin{pmatrix}
                        \boldsymbol{\mu}_X \\
                        \boldsymbol{\mu}_Y \\
                \end{pmatrix}, \quad
                \mathrm{cov}\left\{
                \begin{pmatrix}
                        \mathbf{X} \\
                        \mathbf{Y} \\
                \end{pmatrix}\right\} =
                \begin{pmatrix}
                        \mathbf{\Sigma}_{XX} & \mathbf{\Sigma}_{XY} \\
                        \mathbf{\Sigma}_{YX} & \mathbf{\Sigma}_{YY} \\
                \end{pmatrix}
$$

Let $1 \leq t \leq \mathrm{min}\left(r, s\right)$. $\mathbf{X}$ and $\mathbf{Y}$ are linearly projected into new vector variates,

$$
                \boldsymbol{\xi}_{t \times 1} = \mathbf{G}_{t \times r} \mathbf{X}_{r \times 1}, \quad \boldsymbol{\omega}_{t \times 1} = \mathbf{H}_{t \times s} \mathbf{X}_{s \times 1}
$$

Finding a solution to the problem

$$
                \boldsymbol{\xi} \approx \boldsymbol{\nu} + \boldsymbol{\omega}
$$


requires minimizing the least-squares criterion

$$
        \begin{aligned}
                \mathrm{E}\left[\left(\mathbf{H}\mathbf{Y} - \boldsymbol{\nu} - \mathbf{G}\mathbf{X}\right)\left(\mathbf{H}\mathbf{Y} - \boldsymbol{\nu} - \mathbf{G}\mathbf{X}\right)^\tau\right]
        \end{aligned},
$$

where we assume $\mathrm{cov}\left(\boldsymbol{\omega}\right) = \mathbf{I}_t$. This least-squares criterion is minimized when

$$
                \boldsymbol{\nu}^{\left(t\right)} & = \mathbf{H}^{\left(t\right)} \boldsymbol{\mu}_Y - \mathbf{G}^{\left(t\right)}\boldsymbol{\mu}_Y \\
                \mathbf{G}^{\left(t\right)} & =
                \begin{pmatrix}
                        \mathbf{v}_1^\tau \\
                        \vdots \\
                        \mathbf{v}_t^\tau \\
                \end{pmatrix}
                \mathbf{\Sigma}_{YY}^{-1/2} \mathbf{\Sigma}_{YX}\mathbf{\Sigma}_{XX}^{-1} \\
                \mathbf{H}^{\left(t\right)} & =
                \begin{pmatrix}

 \mathbf{v}_1^\tau \\
                        \vdots \\
                        \mathbf{v}_t^\tau \\
                \end{pmatrix}
                \mathbf{\Sigma}_{YY}^{-1/2} \\
$$

where $\mathbf{v}_j$ is the eigenvector associated with the $j$th largest eigenvalue of

$$
                \mathbf{R} & = \mathbf{\Sigma}_{YY}^{-1/2}\mathbf{\Sigma}_{YX} \mathbf{\Sigma}_{XX}^{-1} \mathbf{\Sigma}_{XY} \mathbf{\Sigma}_{YY}^{-1/2}

B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
$$
